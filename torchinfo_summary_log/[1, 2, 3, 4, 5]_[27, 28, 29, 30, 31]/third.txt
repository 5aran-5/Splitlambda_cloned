Third  : [27, 28, 29, 30, 31]

ThirdLlamaForCausalLM(
  (model): ThirdLlamaModel(
    (embed_tokens): ExtendedIdentity()
    (layers): ModuleList(
      (0-26): 27 x ExtendedIdentity()
      (27-31): 5 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)

===========================================================================================================================================================
Layer (type:depth-idx)                                  Output Shape                                       Param #
===========================================================================================================================================================
ThirdLlamaForCausalLM                                   [1, 32, 50, 128]                                   --
├─ThirdLlamaModel: 1-1                                  [1, 32, 50, 128]                                   --
│    └─ModuleList: 2-1                                  --                                                 --
│    │    └─LlamaDecoderLayer: 3-1                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-1                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-2                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-1                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-2                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-3                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-4         [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-5                       [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-3                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-4                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-6                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-7               [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-8                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-9                       [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-2                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-5                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-6                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-10                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-11                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-12                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-13        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-14                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-7                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-8                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-15                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-16              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-17                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-18                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-3                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-9                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-10                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-19                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-20                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-21                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-22        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-23                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-11                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-12                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-24                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-25              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-26                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-27                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-4                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-13                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-14                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-28                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-29                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-30                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-31        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-32                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-15                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-16                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-33                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-34              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-35                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-36                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-5                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-17                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-18                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-37                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-38                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-39                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-40        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-41                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-19                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-20                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-42                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-43              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-44                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-45                      [1, 50, 4096]                                      45,088,768
│    └─LlamaRMSNorm: 2-2                                [1, 50, 4096]                                      4,096
├─Linear: 1-2                                           [1, 50, 32000]                                     131,072,000
===========================================================================================================================================================
Total params: 1,142,992,896
Trainable params: 1,142,992,896
Non-trainable params: 0
Total mult-adds (G): 1.14
===========================================================================================================================================================
Input size (MB): 0.41
Forward/backward pass size (MB): 57.91
Params size (MB): 2285.99
Estimated Total Size (MB): 2344.30
===========================================================================================================================================================