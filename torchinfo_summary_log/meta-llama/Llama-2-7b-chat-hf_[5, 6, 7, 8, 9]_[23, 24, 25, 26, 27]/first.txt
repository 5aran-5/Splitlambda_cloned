First  : [0, 1, 2, 3, 4, 5, 6, 7, 8]

FirstLlamaForCausalLM(
  (model): FirstLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-8): 9 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-31): 23 x ExtendedIdentity()
    )
    (norm): ExtendedIdentity()
  )
  (lm_head): ExtendedIdentity()
)

===========================================================================================================================================================
Layer (type:depth-idx)                                  Output Shape                                       Param #
===========================================================================================================================================================
FirstLlamaForCausalLM                                   [1, 50, 4096]                                      --
├─FirstLlamaModel: 1-1                                  [1, 50, 4096]                                      --
│    └─Embedding: 2-1                                   [1, 50, 4096]                                      131,072,000
│    └─ModuleList: 2-2                                  --                                                 --
│    │    └─LlamaDecoderLayer: 3-1                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-1                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-2                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-1                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-2                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-3                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-4         [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-5                       [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-3                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-4                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-6                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-7               [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-8                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-9                       [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-2                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-5                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-6                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-10                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-11                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-12                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-13        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-14                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-7                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-8                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-15                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-16              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-17                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-18                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-3                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-9                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-10                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-19                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-20                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-21                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-22        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-23                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-11                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-12                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-24                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-25              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-26                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-27                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-4                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-13                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-14                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-28                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-29                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-30                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-31        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-32                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-15                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-16                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-33                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-34              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-35                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-36                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-5                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-17                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-18                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-37                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-38                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-39                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-40        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-41                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-19                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-20                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-42                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-43              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-44                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-45                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-6                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-21                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-22                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-46                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-47                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-48                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-49        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-50                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-23                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-24                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-51                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-52              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-53                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-54                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-7                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-25                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-26                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-55                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-56                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-57                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-58        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-59                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-27                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-28                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-60                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-61              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-62                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-63                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-8                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-29                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-30                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-64                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-65                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-66                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-67        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-68                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-31                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-32                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-69                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-70              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-71                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-72                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-9                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-33                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-34                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-73                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-74                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-75                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-76        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-77                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-35                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-36                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-78                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-79              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-80                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-81                      [1, 50, 4096]                                      45,088,768
===========================================================================================================================================================
Total params: 1,952,522,240
Trainable params: 1,952,522,240
Non-trainable params: 0
Total mult-adds (G): 1.95
===========================================================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 184.12
Params size (MB): 7810.09
Estimated Total Size (MB): 7994.20
===========================================================================================================================================================