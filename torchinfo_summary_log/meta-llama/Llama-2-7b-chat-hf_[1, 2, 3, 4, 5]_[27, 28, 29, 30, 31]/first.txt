First  : [0, 1, 2, 3, 4]

FirstLlamaForCausalLM(
  (model): FirstLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-31): 27 x ExtendedIdentity()
    )
    (norm): ExtendedIdentity()
  )
  (lm_head): ExtendedIdentity()
)

===========================================================================================================================================================
Layer (type:depth-idx)                                  Output Shape                                       Param #
===========================================================================================================================================================
FirstLlamaForCausalLM                                   [1, 50, 4096]                                      --
├─FirstLlamaModel: 1-1                                  [1, 50, 4096]                                      --
│    └─Embedding: 2-1                                   [1, 50, 4096]                                      131,072,000
│    └─ModuleList: 2-2                                  --                                                 --
│    │    └─LlamaDecoderLayer: 3-1                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-1                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-2                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-1                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-2                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-3                       [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-4         [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-5                       [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-3                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-4                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-6                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-7               [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-8                       [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-9                       [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-2                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-5                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-6                    [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-10                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-11                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-12                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-13        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-14                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-7                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-8                          [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-15                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-16              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-17                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-18                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-3                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-9                      [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-10                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-19                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-20                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-21                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-22        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-23                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-11                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-12                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-24                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-25              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-26                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-27                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-4                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-13                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-14                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-28                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-29                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-30                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-31        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-32                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-15                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-16                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-33                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-34              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-35                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-36                      [1, 50, 4096]                                      45,088,768
│    │    └─LlamaDecoderLayer: 3-5                      [1, 50, 4096]                                      --
│    │    │    └─LlamaRMSNorm: 4-17                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaAttention: 4-18                   [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-37                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-38                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─Linear: 5-39                      [1, 50, 4096]                                      16,777,216
│    │    │    │    └─LlamaRotaryEmbedding: 5-40        [1, 1, 50, 128]                                    --
│    │    │    │    └─Linear: 5-41                      [1, 50, 4096]                                      16,777,216
│    │    │    └─LlamaRMSNorm: 4-19                     [1, 50, 4096]                                      4,096
│    │    │    └─LlamaMLP: 4-20                         [1, 50, 4096]                                      --
│    │    │    │    └─Linear: 5-42                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─SiLUActivation: 5-43              [1, 50, 11008]                                     --
│    │    │    │    └─Linear: 5-44                      [1, 50, 11008]                                     45,088,768
│    │    │    │    └─Linear: 5-45                      [1, 50, 4096]                                      45,088,768
===========================================================================================================================================================
Total params: 1,142,988,800
Trainable params: 1,142,988,800
Non-trainable params: 0
Total mult-adds (G): 1.14
===========================================================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 103.01
Params size (MB): 4571.96
Estimated Total Size (MB): 4674.97
===========================================================================================================================================================